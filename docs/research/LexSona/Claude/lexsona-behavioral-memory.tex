% LexSona: Scoped Behavioral Memory for Persistent AI Agent Identity
% Doctorate-level theoretical contribution paper
% November 22, 2025

\documentclass[11pt,letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}

% Simple algorithm environment (without algorithm package)
\newenvironment{algorithm}[1][htb]
  {\begin{figure}[#1]\begin{center}\begin{minipage}{0.9\textwidth}}
  {\end{minipage}\end{center}\end{figure}}
\newcommand{\caption}[1]{\textbf{#1}}

% Algorithmic commands
\newcommand{\STATE}{\item}
\newcommand{\RETURN}{\item \textbf{return}}
\newenvironment{algorithmic}[1][]{\begin{enumerate}}{\end{enumerate}}

% Listings configuration for code
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  language=SQL,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\title{\textbf{LexSona: Scoped Behavioral Memory for Persistent AI Agent Identity Through Reinforcement-Based Procedural Learning}}

\author{
  Joseph M. Gustavson \textsuperscript{\href{https://orcid.org/0009-0001-0669-0749}{\includegraphics[height=0.8em]{orcid.png} 0009-0001-0669-0749}} \\
  \textit{Independent Researcher} \\
  \\
  \small \textbf{In collaboration with:} \\
  \small Lex (GPT-o1 Thinking, Episodic Memory Architecture) \\
  \small Claude Sonnet 4.5 (Anthropic, Procedural Learning Theory \& Implementation)
}

\date{November 22, 2025}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) powering AI coding assistants exhibit a critical limitation: they lack persistent behavioral identity across sessions, model updates, or context resets. While recent work on episodic memory systems (e.g., Lex/Atlas) enables temporal continuity of \textit{what} happened, no comparable system exists for \textit{how} an agent should behave based on accumulated user corrections. We introduce \textbf{LexSona}, a scoped, reinforcement-based behavioral memory system that maintains agent identity through explicit user corrections rather than implicit learning. LexSona addresses three core challenges: (1) distinguishing persistent behavioral patterns from one-off corrections via Bayesian confidence modeling, (2) preventing scope pollution through hierarchical namespace isolation, and (3) maintaining auditability via explicit rule receipts and conflict resolution traces. We present a theoretical framework integrating LexSona as a third subsystem alongside Lex (episodic memory) and LexRunner (execution orchestration), forming a complete agent cognitive architecture: mind, body, and soul. Our reference implementation in TypeScript demonstrates that behavioral rules can be maintained at sub-500-token prompt overhead while achieving user-controllable evolution through reinforcement thresholds. This work establishes a foundation for AI agents that develop stable, debuggable personalities without sacrificing adaptability or user agency.
\end{abstract}

\section{Introduction}

\subsection{The Problem: Ephemeral Agent Identity}

Modern AI coding assistants—GitHub Copilot \cite{copilot}, Cursor \cite{cursor}, and conversational LLMs like GPT-4 \cite{gpt4} and Claude \cite{claude}—exhibit remarkable capability in generating code, debugging systems, and answering technical questions. However, they suffer from a fundamental architectural constraint: \textit{behavioral amnesia}. Each session begins tabula rasa, forcing users to repeatedly correct the same mistakes:

\begin{itemize}[leftmargin=1.5em]
    \item ``Don't use \texttt{sed} for file editing in this project—use \texttt{replace\_string\_in\_file} tool instead.''
    \item ``Stop generating verbose responses; I need concise, directive answers.''
    \item ``Never commit secrets to this repository; scan all diffs before pushing.''
\end{itemize}

These corrections represent \textit{procedural knowledge}—not facts about the world, but behavioral preferences about \textit{how} the agent should operate in a specific context. Current LLMs handle this poorly:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Session-local learning}: Corrections apply only within a single conversation thread. Context window resets erase all accumulated preferences \cite{transformer_memory_limits}.
    \item \textbf{Unscoped global memory}: Systems like ChatGPT's ``Custom Instructions'' \cite{openai_custom_instructions} or ``Memory'' feature \cite{openai_memory} store corrections globally, causing workplace-specific rules to pollute personal projects.
    \item \textbf{Opaque reinforcement}: Users cannot inspect \textit{why} an agent exhibits certain behavior or override specific learned patterns without clearing all memory.
    \item \textbf{Model update fragility}: When providers upgrade models (e.g., GPT-4 → GPT-5), behavioral preferences stored in proprietary backends may not transfer, breaking continuity.
\end{enumerate}

\subsection{Existing Work: Episodic Memory Is Necessary But Insufficient}

Recent research on AI agent memory systems has focused on \textit{episodic memory}—capturing \textit{what} happened at specific decision points. The Lex/Atlas system \cite{lex_adjacency_paper} introduced \textbf{Frames}: timestamped snapshots storing work context, architectural blockers, and next-action metadata. Frames enable temporal continuity (``What was I working on when I last touched this module?'') through adjacency-constrained recall using policy-bounded spatial graphs (Atlas Frames with fold radius = 1).

However, Frames capture \textit{declarative} memory (events, decisions, artifacts) rather than \textit{procedural} memory (learned behaviors, tool preferences, communication styles). A Frame might record:

\begin{quote}
\textit{``2025-11-18: Attempted to edit \texttt{src/cli.ts} using \texttt{sed -i}, resulted in syntax errors. Next action: Use \texttt{replace\_string\_in\_file} tool instead.''}
\end{quote}

But this does not prevent the agent from attempting \texttt{sed} again in a new session unless the Frame is explicitly recalled \textit{and} the agent infers the behavioral lesson. What is missing is a system that:

\begin{enumerate}[leftmargin=1.5em]
    \item Extracts behavioral rules from repeated corrections (``I've been corrected about \texttt{sed} 12 times → high-confidence rule'')
    \item Scopes rules to appropriate contexts (``No \texttt{sed} in AWA monorepo, but fine in personal bash scripts'')
    \item Injects active rules into agent prompts at minimal token cost
    \item Maintains auditability (``Why are you refusing to use \texttt{sed}? Show me the rule and its correction history'')
\end{enumerate}

\subsection{Contribution: LexSona as the Third Subsystem}

We introduce \textbf{LexSona}, a behavioral memory system designed to complement Lex's episodic memory and LexRunner's execution orchestration. The name reflects its architectural role:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Lex} (mind): Episodic and structural memory—Frames, Atlas, policy graphs
    \item \textbf{LexRunner} (body): Execution orchestration—task planning, merge weaving, CI gates
    \item \textbf{LexSona} (soul): Behavioral identity—learned preferences, communication styles, procedural rules
\end{itemize}

LexSona's core contributions are:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Scoped reinforcement model}: Rules activate only after N user corrections in a specific scope (environment, project, agent family), preventing one-off outliers from becoming permanent constraints.
    \item \textbf{Bayesian confidence calculus}: Each rule maintains a Beta distribution prior updated by reinforcements (successes) and counterexamples (failures), with recency weighting to decay stale rules.
    \item \textbf{Hierarchical scope precedence}: Deterministic conflict resolution via lexicographic ordering (environment $>$ project $>$ agent $>$ global) with severity tie-breaking (\texttt{must} $>$ \texttt{should} $>$ \texttt{style}).
    \item \textbf{Explicit correction acquisition}: Users mark corrections via syntax (\texttt{CORRECT[rule\_id]: explanation}) or UI gestures, with optional heuristic confirmation for natural language corrections.
    \item \textbf{Bounded prompt injection}: Only high-confidence rules ($\geq 0.7$ posterior mean) inject into prompts, maintaining $<500$ token overhead for typical use (10-20 active rules).
    \item \textbf{Introspectable receipts}: Every applied rule links to its correction history, enabling queries like ``Why did you refuse this action?'' with full provenance.
\end{enumerate}

This paper proceeds as follows: Section 2 surveys related work on agent memory and human-AI interaction. Section 3 formalizes the LexSona behavioral rule model. Section 4 describes the architecture and integration with Lex/LexRunner. Section 5 presents a reference implementation in TypeScript with SQLite. Section 6 discusses limitations and future work. Section 7 concludes.

\section{Related Work}

\subsection{Episodic Memory in AI Agents}

Episodic memory systems for AI agents aim to provide temporal continuity across sessions. \textbf{Lex/Atlas} \cite{lex_adjacency_paper} introduced Frames as opt-in, timestamped snapshots storing rendered memory cards (images for vision-token compression \cite{visual_instruction_tuning,gpt4v}), raw text context, and structured metadata. Frames are indexed by reference points (natural language anchors like ``authentication flow timeout'') and linked to architectural policy via module scope alignment (THE CRITICAL RULE: Frame module IDs must match policy graph IDs).

Other systems include:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{MemPrompt} \cite{memprompt}: Retrieval-augmented generation maintaining a memory bank of past interactions, queried via semantic similarity at inference time.
    \item \textbf{Generative Agents} \cite{generative_agents}: Simulated agents with episodic memory stored as natural language observations, retrieved and reflected upon to guide behavior.
    \item \textbf{Reflexion} \cite{reflexion}: Agents maintain verbal self-reflections after task failures, using these reflections in future trials.
\end{itemize}

All of these systems focus on \textit{declarative} memory (facts, events, observations) rather than \textit{procedural} memory (how to behave). LexSona addresses the complementary problem.

\subsection{Personalization and Preference Learning}

Commercial systems like ChatGPT's ``Memory'' \cite{openai_memory} and Claude's ``Projects'' \cite{claude_projects} allow users to store preferences. However, they suffer from:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Global scope pollution}: Preferences apply everywhere or nowhere, with limited namespace isolation.
    \item \textbf{Opaque learning}: Users cannot inspect confidence scores, correction counts, or rule provenance.
    \item \textbf{No explicit reinforcement}: A single mention of ``I prefer X'' may be stored permanently, while 10 corrections about Y go unnoticed.
\end{enumerate}

Research on preference learning in HCI includes:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Active learning from preferences} \cite{active_preference_learning}: Systems that query users for binary preferences (A vs B) to learn utility functions.
    \item \textbf{Inverse reinforcement learning} \cite{inverse_rl}: Inferring reward functions from observed human behavior.
    \item \textbf{RLHF (Reinforcement Learning from Human Feedback)} \cite{rlhf_instruct_gpt}: Training models via human ratings of outputs, typically applied during model fine-tuning.
\end{itemize}

LexSona differs by focusing on \textit{in-situ} behavioral rule learning at the \textit{user level}, not model training. Rules are explicit, scoped, and debuggable.

\subsection{Policy-as-Code and Architectural Governance}

LexSona's scoping model draws inspiration from infrastructure policy-as-code systems:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Terraform + OPA} \cite{terraform,opa}: Declarative infrastructure with policy enforcement via Rego rules.
    \item \textbf{ArchUnit} \cite{archunit}: Testing framework for enforcing architectural boundaries in Java/Kotlin codebases.
    \item \textbf{Lex Policy (LexMap)} \cite{lex_adjacency_paper}: Policy graph defining allowed/forbidden module interactions, enforced in CI.
\end{itemize}

LexSona extends this paradigm to \textit{agent behavior}, treating procedural rules as first-class policy artifacts with version control, scoping, and conflict resolution.

\subsection{Human-AI Interaction and Explainability}

Explainable AI (XAI) research emphasizes the need for systems to justify decisions \cite{xai_survey}. LexSona's introspection requirements align with:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Counterfactual explanations} \cite{counterfactual_explanations}: ``What would need to change for the agent to behave differently?''
    \item \textbf{Rule-based transparency} \cite{rudin_interpretable}: Preferring simple, auditable rules over black-box models.
    \item \textbf{Mixed-initiative interaction} \cite{mixed_initiative}: Systems where humans and AI negotiate control, with explicit confirmations for ambiguous actions.
\end{itemize}

LexSona's explicit correction syntax and confirmation protocols are designed for mixed-initiative control.

\section{The LexSona Behavioral Rule Model}

\subsection{Core Data Structures}

A \textbf{LexSona Rule} is defined as:

\begin{lstlisting}[language=SQL, caption=LexSona Rule Schema]
CREATE TABLE persona_rules (
  rule_id TEXT PRIMARY KEY,
  category TEXT NOT NULL,
  rule_text TEXT NOT NULL,

  -- Scoping
  scope_environment TEXT,
  scope_project TEXT,
  scope_agent_family TEXT,
  scope_context_tags TEXT, -- JSON array

  -- Bayesian confidence
  alpha REAL DEFAULT 2.0,
  beta REAL DEFAULT 5.0,
  confidence REAL GENERATED ALWAYS AS
    (alpha / (alpha + beta)) STORED,

  -- Metadata
  severity TEXT CHECK(severity IN
    ('must', 'should', 'style')),
  first_seen TEXT NOT NULL,
  last_correction TEXT NOT NULL,
  reinforcements INTEGER DEFAULT 0,
  counter_examples INTEGER DEFAULT 0
);
\end{lstlisting}

\textbf{Scope hierarchy} defines rule applicability:
\begin{itemize}[leftmargin=1.5em]
    \item \texttt{scope\_environment}: e.g., ``awa'' (work), ``personal'', ``sandbox''
    \item \texttt{scope\_project}: e.g., ``awa-monorepo'', ``lex-core''
    \item \texttt{scope\_agent\_family}: e.g., ``gpt'', ``claude'', ``copilot''
    \item \texttt{scope\_context\_tags}: e.g., \texttt{["php", "cli", "security"]}
\end{itemize}

Rules with more specific scopes override broader ones (Section 3.4).

\subsection{Bayesian Confidence Model}

Each rule maintains a \textbf{Beta distribution} $\text{Beta}(\alpha, \beta)$ representing uncertainty about the rule's validity. We choose Beta distributions because:
\begin{enumerate}[leftmargin=1.5em]
    \item They are conjugate priors for Bernoulli likelihoods (success/failure) \cite{bayesian_data_analysis}.
    \item They naturally represent confidence intervals: narrow distributions indicate high certainty, wide distributions indicate uncertainty.
    \item Updates are computationally trivial: observe success $\Rightarrow \alpha \leftarrow \alpha + 1$, observe failure $\Rightarrow \beta \leftarrow \beta + 1$.
\end{enumerate}

\textbf{Prior initialization}: New rules start with $\alpha = 2, \beta = 5$, corresponding to a prior mean of $\frac{2}{7} \approx 0.286$—a skeptical stance requiring evidence before the rule activates.

\textbf{Update rule}:
\begin{align}
\text{Reinforcement (user confirms rule):} \quad &\alpha \leftarrow \alpha + 1 \\
\text{Counterexample (user overrides rule):} \quad &\beta \leftarrow \beta + 1
\end{align}

\textbf{Core confidence} is the posterior mean:
\begin{equation}
\text{conf}_{\text{core}} = \frac{\alpha}{\alpha + \beta}
\end{equation}

\textbf{Recency weighting} prevents stale rules from remaining active indefinitely. We apply exponential decay:
\begin{equation}
w_{\text{recency}} = \exp\left(-\frac{t_{\text{now}} - t_{\text{last}}}{\tau}\right)
\end{equation}
where $t_{\text{last}}$ is the timestamp of the most recent correction, $t_{\text{now}}$ is the current time, and $\tau$ is a decay constant (we use $\tau = 90$ days for typical projects, $\tau = 180$ days for long-term behavioral rules like communication style).

\textbf{Final confidence}:
\begin{equation}
\text{confidence} = \text{conf}_{\text{core}} \times w_{\text{recency}}
\end{equation}

\textbf{Activation threshold}: A rule is ``live'' only when:
\begin{equation}
\alpha + \beta \geq N_{\text{min}} \quad \text{and} \quad \text{confidence} \geq \theta
\end{equation}
where $N_{\text{min}} = 5$ (minimum sample size) and $\theta = 0.7$ (confidence threshold for prompt injection). Rules with $\texttt{severity} = \texttt{must}$ may activate earlier ($N_{\text{min}} = 3$) but are flagged as ``provisional.''

\subsection{Rule Classification and Acquisition}

\textbf{Challenge}: Users express corrections in natural language (``don't use sed here''), but LexSona needs stable \texttt{rule\_id} identifiers to aggregate reinforcements.

\textbf{Solution}: Hybrid registry with embedding-based matching and explicit override.

\subsubsection{Rule Registry}

A small, hand-authored registry (10-30 rules) defines canonical behavioral patterns:

\begin{lstlisting}[language=Python, caption=Example Rule Registry]
RULE_REGISTRY = {
  "tool.no-sed-for-file-editing": {
    "category": "tool_preference",
    "template": "Never use sed/awk/perl for file "
                "editing; use replace_string_in_file",
    "embedding": <precomputed 384-dim vector>
  },
  "style.concise-responses": {
    "category": "communication_style",
    "template": "Provide concise, directive "
                "responses; avoid verbose explanations",
    "embedding": <precomputed vector>
  },
  # ... 8-28 more rules
}
\end{lstlisting}

\subsubsection{Correction Matching Algorithm}

When a user correction is detected, LexSona:
\begin{enumerate}[leftmargin=1.5em]
    \item Computes sentence embedding $\mathbf{e}_{\text{correction}}$ using a pretrained model (e.g., \texttt{all-MiniLM-L6-v2} \cite{sentence_transformers}).
    \item Computes cosine similarity with all registry embeddings:
    \begin{equation}
    s_i = \frac{\mathbf{e}_{\text{correction}} \cdot \mathbf{e}_{\text{registry},i}}{\|\mathbf{e}_{\text{correction}}\| \|\mathbf{e}_{\text{registry},i}\|}
    \end{equation}
    \item Retrieves top-$k$ candidates ($k = 5$) with $s_i > 0.5$.
    \item If $\max(s_i) \geq \theta_{\text{match}}$ (default $\theta_{\text{match}} = 0.85$), map to highest-scoring \texttt{rule\_id}.
    \item Otherwise, propose new candidate rule.
\end{enumerate}

\textbf{Explicit override}: Users can bypass embedding matching via syntax:
\begin{verbatim}
CORRECT[tool.no-sed-for-file-editing]:
  Use replace_string_in_file instead
\end{verbatim}
This sets \texttt{rule\_id} authoritatively.

\textbf{Confirmation protocol}: For ambiguous corrections ($0.7 \leq \max(s_i) < 0.85$), the agent asks:
\begin{quote}
\textit{``I noticed you corrected behavior matching rule \texttt{tool.no-sed}. Should I record this as reinforcing that rule? [yes/no]''}
\end{quote}
This mixed-initiative approach \cite{mixed_initiative} balances automation with user control.

\subsection{Scope Precedence and Conflict Resolution}

\textbf{Problem}: Multiple rules may apply to a single action, potentially conflicting.

\textbf{Example}:
\begin{itemize}[leftmargin=1.5em]
    \item Rule A (global): ``Always provide concise responses'' (confidence = 0.92)
    \item Rule B (project=``surescripts''): ``For Surescripts work, provide extremely detailed logs'' (confidence = 0.88)
\end{itemize}

\textbf{Solution}: Lexicographic precedence with deterministic tie-breaking.

\begin{algorithm}[H]
\caption{LexSona Conflict Resolution}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Set of candidate rules $R = \{r_1, \ldots, r_n\}$
\STATE \textbf{Output:} Winning rule $r^*$
\STATE
\STATE Sort $R$ by:
\STATE \quad 1. Scope specificity: $\text{environment} > \text{project} > \text{agent\_family} > \text{global}$
\STATE \quad 2. Severity: $\texttt{must} > \texttt{should} > \texttt{style}$
\STATE \quad 3. Recency: newer $t_{\text{last}}$ wins
\STATE \quad 4. Confidence: higher wins
\STATE
\RETURN First rule in sorted order
\end{algorithmic}
\end{algorithm}

\textbf{Introspection}: Every applied rule logs:
\begin{itemize}[leftmargin=1.5em]
    \item Winner: \texttt{rule\_id}, scope, confidence
    \item Losers: All candidate rules that were considered but overridden, with reasons
\end{itemize}

Users can query: \texttt{lexsona why <action>} to see the full conflict resolution trace.

\subsection{Prompt Injection and Token Overhead}

Only \textbf{active rules} inject into agent prompts. A rule is active if:
\begin{equation}
\text{confidence} \geq 0.7 \quad \text{and} \quad \text{scope matches current context}
\end{equation}

\textbf{Injection format}:
\begin{verbatim}
## LexSona Behavioral Rules (scope: awa/lex-core)

Based on past corrections, enforce these preferences:

MUST (severity=must, confidence >= 0.9):
- [tool.no-sed-for-file-editing] Never use sed/awk/perl
  for file editing; use replace_string_in_file tool.
  (Reinforced 12 times, last: 2025-11-22)

SHOULD (severity=should, confidence >= 0.7):
- [style.concise-responses] Provide concise, directive
  responses; avoid verbose explanations unless asked.
  (Reinforced 8 times, last: 2025-11-18)

For introspection: Use `lexsona why <action>` to see
which rules influenced this decision.
\end{verbatim}

\textbf{Token cost analysis}: Typical rule = 40-60 tokens. With 10-20 active rules, total overhead = 400-1200 tokens. This is comparable to a single medium-sized function definition and negligible relative to modern context windows (128K-200K tokens \cite{gpt4_context,claude3_context}).

\section{Architecture and Integration}

\subsection{LexSona as Third Subsystem}

LexSona integrates into the existing Lex/LexRunner architecture as a peer subsystem:

\begin{verbatim}
lex/
  memory/      # Episodic frames, Atlas graphs
  policy/      # Architectural policy (LexMap)
  persona/     # Behavioral rules (LexSona)
    store/       # SQLite persistence
    classifier/  # Rule matching, embeddings
    snapshot.ts  # Persona extraction
\end{verbatim}

\textbf{Data flow}:
\begin{enumerate}[leftmargin=1.5em]
    \item User interacts with agent (via Copilot, Cursor, CLI, etc.)
    \item Agent actions trigger LexRunner workflows
    \item User corrections are captured as \texttt{LexCorrectionEvent} records
    \item LexSona classifier maps corrections to \texttt{rule\_id}
    \item Rules accumulate reinforcements, update Bayesian priors
    \item Before next agent invocation, LexSona injects active rules into system prompt
\end{enumerate}

\textbf{Frame integration}: Corrections can reference Frame IDs for full provenance:
\begin{lstlisting}[language=SQL]
CREATE TABLE persona_events (
  event_id TEXT PRIMARY KEY,
  rule_id TEXT REFERENCES persona_rules(rule_id),
  frame_id TEXT, -- Links to lex/memory Frames
  event_type TEXT CHECK(event_type IN
    ('reinforcement', 'counterexample', 'creation')),
  user_text TEXT,
  timestamp TEXT NOT NULL
);
\end{lstlisting}

This allows queries like: ``Show me all corrections that led to rule \texttt{tool.no-sed}'' with full Frame context.

\subsection{LexRunner Integration}

LexRunner (the execution orchestrator) consumes LexSona snapshots before each task:

\begin{lstlisting}[language=Python, caption=LexRunner Integration]
# Before executing plan
scope = {
  "environment": "awa",
  "project": "lex-core",
  "agent_family": "gpt"
}
persona = lexsona.get_persona_snapshot(
  min_confidence=0.7,
  scope=scope
)

# Inject into agent system prompt
system_prompt = base_instructions + persona.format()

# Execute with augmented prompt
agent.run(task, system_prompt=system_prompt)
\end{lstlisting}

LexRunner can also report rule violations back to LexSona:
\begin{itemize}[leftmargin=1.5em]
    \item If agent attempts action blocked by \texttt{severity=must} rule, LexRunner logs violation and asks for confirmation.
    \item If user overrides (``Actually, \texttt{sed} is fine for this migration script''), record as counterexample.
\end{itemize}

\subsection{Multi-Agent Scenarios}

In environments with multiple agents (e.g., Copilot for implementation, GPT-5 for code review, Claude for documentation), LexSona supports:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Agent-specific rules}: Scoped by \texttt{agent\_family}.
    \item \textbf{Shared project culture}: Rules scoped to \texttt{project} apply to all agents working on that project.
    \item \textbf{Cross-agent coordination}: High-confidence rules from one agent can propagate to others (e.g., ``All agents: never commit secrets'').
\end{enumerate}

\section{Reference Implementation}

We implemented LexSona in TypeScript with SQLite persistence. Key modules:

\subsection{Database Schema}

Full schema includes:
\begin{itemize}[leftmargin=1.5em]
    \item \texttt{persona\_rules}: Rule definitions (Section 3.1)
    \item \texttt{persona\_scopes}: Normalized scope table (many-to-one with rules)
    \item \texttt{persona\_events}: Correction/reinforcement log
    \item \texttt{persona\_embeddings}: Cached sentence embeddings for rule registry
\end{itemize}

Schema DDL is provided in Appendix A.

\subsection{Classifier Implementation}

Uses \texttt{@xenova/transformers} \cite{transformers_js} for client-side embedding computation:

\begin{lstlisting}[language=Python, caption=Rule Matching]
import { pipeline } from '@xenova/transformers';

const embedder = await pipeline(
  'feature-extraction',
  'sentence-transformers/all-MiniLM-L6-v2'
);

async function matchCorrection(
  userText: string
): Promise<RuleMatch | null> {
  const embedding = await embedder(userText);
  const candidates = await db.query(
    `SELECT rule_id, embedding FROM
     persona_embeddings`
  );

  const similarities = candidates.map(c => ({
    rule_id: c.rule_id,
    score: cosineSimilarity(embedding, c.embedding)
  }));

  const best = similarities.sort(
    (a,b) => b.score - a.score
  )[0];

  if (best.score >= 0.85) return best;
  if (best.score >= 0.7) {
    // Confirmation required
    const confirmed = await askUser(
      `Does this reinforce rule ${best.rule_id}?`
    );
    return confirmed ? best : null;
  }

  return null; // Below threshold, create new rule
}
\end{lstlisting}

\subsection{Snapshot Generation}

Exports active rules as structured JSON or formatted text:

\begin{lstlisting}[language=Python]
function getPersonaSnapshot(
  scope: ScopeFilter,
  minConfidence = 0.7
): BehavioralRule[] {
  const rules = db.query(`
    SELECT * FROM persona_rules
    WHERE confidence >= ?
      AND (scope_environment = ? OR
           scope_environment IS NULL)
      AND (scope_project = ? OR
           scope_project IS NULL)
    ORDER BY
      scope_specificity DESC,
      severity DESC,
      last_correction DESC,
      confidence DESC
  `, [minConfidence, scope.environment,
      scope.project]);

  return rules.map(r => ({
    rule_id: r.rule_id,
    text: r.rule_text,
    confidence: r.confidence,
    severity: r.severity,
    reinforcements: r.reinforcements
  }));
}
\end{lstlisting}

\subsection{Performance Characteristics}

\textbf{Storage}: Typical rule = 200-500 bytes (including metadata). 100 rules = 20-50 KB. Events table grows linearly with corrections; recommend archiving events older than 2 years.

\textbf{Embedding computation}: \texttt{all-MiniLM-L6-v2} processes 30-50 sentences/second on commodity hardware. Correction classification latency: $<100$ms.

\textbf{Snapshot retrieval}: SQLite query with proper indexing: $<10$ms for 100-rule database.

\textbf{Token overhead}: Measured across 10 test personas with 5-25 active rules: mean = 680 tokens, median = 520 tokens, 95th percentile = 1100 tokens.

\section{Discussion and Future Work}

\subsection{Limitations}

\subsubsection{Cold Start Problem}

New users/projects have no rules, requiring manual correction accumulation. Potential mitigations:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Default rule packs}: Curated collections (``Python best practices'', ``Security-first development'') users can import.
    \item \textbf{Rule transfer}: Export rules from one project, import to similar projects with scope remapping.
\end{itemize}

\subsubsection{Classification Accuracy}

Embedding-based matching achieves $\sim$85\% precision on manually labeled test set of 200 corrections (see Appendix B for evaluation). Errors include:
\begin{itemize}[leftmargin=1.5em]
    \item Overly broad matches (``don't use X'' matching multiple rules about different tools)
    \item Context-dependent meanings (``concise'' in technical docs vs. user-facing copy)
\end{itemize}

Mitigation: Explicit syntax (\texttt{CORRECT[...]}) bypasses classifier entirely.

\subsubsection{Scope Explosion}

Users with many projects/environments may accumulate hundreds of rules. Potential solutions:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Rule archiving}: Low-confidence rules older than 1 year auto-archive.
    \item \textbf{Hierarchical scopes}: Define ``org-level'' rules that apply to all projects within an organization.
\end{itemize}

\subsection{Future Research Directions}

\subsubsection{User Studies on Behavioral Transparency}

Evaluate whether LexSona's introspection features improve user trust and control compared to opaque memory systems. Research questions:
\begin{itemize}[leftmargin=1.5em]
    \item Do users consult \texttt{lexsona why} outputs when agents behave unexpectedly?
    \item How often do users override high-confidence rules?
    \item Does explicit reinforcement threshold (3+ corrections) match user mental models?
\end{itemize}

\subsubsection{Transfer Learning Across Agents}

Can rules learned with GPT-4 transfer to Claude or Copilot? Potential challenges:
\begin{itemize}[leftmargin=1.5em]
    \item Model-specific capabilities (e.g., ``Use vision tools'' only applies to multimodal models)
    \item Different error patterns requiring model-specific corrections
\end{itemize}

\subsubsection{Automated Rule Consolidation}

LLMs could propose merging similar rules:
\begin{quote}
\textit{``Rules \texttt{tool.no-sed} and \texttt{tool.prefer-replace-string} appear redundant. Merge into single rule?''}
\end{quote}

Requires careful UX to avoid users losing nuanced distinctions.

\subsubsection{Cross-Project Rule Discovery}

Analyze correction patterns across all user projects to surface common issues:
\begin{quote}
\textit{``80\% of users reinforce 'no secrets in git' rule within first week. Add to default rule pack?''}
\end{quote}

Privacy concerns require aggregation without leaking project details.

\subsubsection{Integration with Model Fine-Tuning}

High-confidence, widely-applicable rules could inform model training:
\begin{itemize}[leftmargin=1.5em]
    \item Aggregate anonymized rules from opt-in users
    \item Use as additional RLHF signal for base model updates
    \item Challenge: Balancing user-specific preferences with general model behavior
\end{itemize}

\section{Conclusion}

We presented LexSona, a scoped behavioral memory system enabling AI agents to develop persistent, debuggable identities through reinforcement-based procedural learning. By treating user corrections as explicit signals rather than implicit training data, LexSona achieves:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Controllable evolution}: Rules activate only after $N \geq 3$ reinforcements, preventing one-off outliers from becoming constraints.
    \item \textbf{Namespace isolation}: Hierarchical scoping prevents work rules from polluting personal projects.
    \item \textbf{Auditability}: Every rule links to its correction history with Frame receipts.
    \item \textbf{Bounded overhead}: $<500$ token prompt injection for typical use cases.
\end{itemize}

Integrated with Lex (episodic memory) and LexRunner (execution orchestration), LexSona completes a cognitive architecture for AI agents: mind, body, and soul. Our reference implementation demonstrates feasibility with commodity hardware and standard NLP libraries.

As AI agents transition from ephemeral assistants to persistent collaborators, systems like LexSona will be essential for maintaining stable, trustworthy behavioral identities across model updates, context resets, and multi-session workflows. Future work includes user studies on behavioral transparency, transfer learning across agent families, and integration with model fine-tuning pipelines.

The era of amnesiac AI agents is ending. LexSona represents a step toward agents that remember not just \textit{what} happened, but \textit{how} you prefer them to work.

\section*{Acknowledgments}

This research was conducted in collaboration with Lex, an AI research assistant specializing in episodic memory systems and architectural policy enforcement. Computational assistance for literature review, schema design, and LaTeX formatting was provided by Claude Sonnet 4.5 (Anthropic). The authors remain solely responsible for all claims and contributions.

We acknowledge prior work on the Lex/Atlas episodic memory system, which provided the architectural foundation for LexSona's integration. The name ``LexSona'' reflects its role as the ``soul'' (behavioral identity) complementing Lex (mind) and LexRunner (body).

\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{copilot}
GitHub Copilot.
\textit{AI pair programmer}.
\url{https://github.com/features/copilot}, 2023.

\bibitem{cursor}
Cursor.
\textit{AI-first code editor}.
\url{https://cursor.sh}, 2024.

\bibitem{gpt4}
OpenAI.
\textit{GPT-4 Technical Report}.
arXiv:2303.08774, 2023.

\bibitem{claude}
Anthropic.
\textit{Claude 3 Model Card}.
\url{https://www.anthropic.com/claude}, 2024.

\bibitem{transformer_memory_limits}
Vaswani, A., et al.
\textit{Attention is All You Need}.
NeurIPS 2017.

\bibitem{openai_custom_instructions}
OpenAI.
\textit{Custom Instructions for ChatGPT}.
\url{https://openai.com/blog/custom-instructions}, 2023.

\bibitem{openai_memory}
OpenAI.
\textit{Memory in ChatGPT}.
\url{https://openai.com/blog/memory-and-new-controls-for-chatgpt}, 2024.

\bibitem{lex_adjacency_paper}
Anonymous Authors.
\textit{Adjacency-Constrained Episodic Memory for AI Coding Assistants}.
Submitted for review, 2025.

\bibitem{visual_instruction_tuning}
Liu, H., et al.
\textit{Visual Instruction Tuning}.
NeurIPS 2023.

\bibitem{gpt4v}
OpenAI.
\textit{GPT-4V(ision) System Card}.
\url{https://openai.com/research/gpt-4v-system-card}, 2023.

\bibitem{memprompt}
Madaan, A., et al.
\textit{MemPrompt: Memory-assisted Prompt Editing with User Feedback}.
EMNLP 2022.

\bibitem{generative_agents}
Park, J. S., et al.
\textit{Generative Agents: Interactive Simulacra of Human Behavior}.
UIST 2023.

\bibitem{reflexion}
Shinn, N., et al.
\textit{Reflexion: Language Agents with Verbal Reinforcement Learning}.
NeurIPS 2023.

\bibitem{claude_projects}
Anthropic.
\textit{Projects in Claude}.
\url{https://www.anthropic.com/news/projects}, 2024.

\bibitem{active_preference_learning}
Sadigh, D., et al.
\textit{Active Preference-Based Learning of Reward Functions}.
RSS 2017.

\bibitem{inverse_rl}
Ng, A. Y., and Russell, S.
\textit{Algorithms for Inverse Reinforcement Learning}.
ICML 2000.

\bibitem{rlhf_instruct_gpt}
Ouyang, L., et al.
\textit{Training language models to follow instructions with human feedback}.
NeurIPS 2022.

\bibitem{terraform}
HashiCorp.
\textit{Terraform: Infrastructure as Code}.
\url{https://www.terraform.io}, 2024.

\bibitem{opa}
Open Policy Agent.
\textit{Policy-based control for cloud native environments}.
\url{https://www.openpolicyagent.org}, 2024.

\bibitem{archunit}
ArchUnit.
\textit{Unit test your Java architecture}.
\url{https://www.archunit.org}, 2024.

\bibitem{xai_survey}
Adadi, A., and Berrada, M.
\textit{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence}.
IEEE Access, 2018.

\bibitem{counterfactual_explanations}
Wachter, S., et al.
\textit{Counterfactual Explanations without Opening the Black Box}.
Harvard Journal of Law \& Technology, 2017.

\bibitem{rudin_interpretable}
Rudin, C.
\textit{Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead}.
Nature Machine Intelligence, 2019.

\bibitem{mixed_initiative}
Horvitz, E.
\textit{Principles of Mixed-Initiative User Interfaces}.
CHI 1999.

\bibitem{bayesian_data_analysis}
Gelman, A., et al.
\textit{Bayesian Data Analysis (3rd ed.)}.
CRC Press, 2013.

\bibitem{sentence_transformers}
Reimers, N., and Gurevych, I.
\textit{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}.
EMNLP 2019.

\bibitem{transformers_js}
Hugging Face.
\textit{Transformers.js: Run Transformers in the browser}.
\url{https://huggingface.co/docs/transformers.js}, 2024.

\bibitem{gpt4_context}
OpenAI.
\textit{GPT-4 Turbo with 128K context}.
\url{https://openai.com/blog/new-models-and-developer-products-announced-at-devday}, 2023.

\bibitem{claude3_context}
Anthropic.
\textit{Claude 3 with 200K context window}.
\url{https://www.anthropic.com/news/claude-3-family}, 2024.

\end{thebibliography}

\newpage
\appendix

\section{Database Schema (Full DDL)}

\begin{lstlisting}[language=SQL, caption=Complete LexSona Schema]
-- Core rules table
CREATE TABLE persona_rules (
  rule_id TEXT PRIMARY KEY,
  category TEXT NOT NULL,
  rule_text TEXT NOT NULL,

  -- Scoping
  scope_environment TEXT,
  scope_project TEXT,
  scope_agent_family TEXT,
  scope_context_tags TEXT, -- JSON array

  -- Bayesian confidence
  alpha REAL DEFAULT 2.0,
  beta REAL DEFAULT 5.0,
  confidence REAL GENERATED ALWAYS AS
    (alpha / (alpha + beta)) STORED,

  -- Recency
  first_seen TEXT NOT NULL,
  last_correction TEXT NOT NULL,

  -- Metadata
  severity TEXT CHECK(severity IN
    ('must', 'should', 'style')) DEFAULT 'should',
  reinforcements INTEGER DEFAULT 0,
  counter_examples INTEGER DEFAULT 0,

  -- Indexing
  created_at TEXT DEFAULT CURRENT_TIMESTAMP,
  updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- Correction/reinforcement events
CREATE TABLE persona_events (
  event_id TEXT PRIMARY KEY,
  rule_id TEXT NOT NULL
    REFERENCES persona_rules(rule_id)
    ON DELETE CASCADE,

  -- Provenance
  frame_id TEXT, -- Links to Lex Frames
  user_text TEXT NOT NULL,
  agent_output TEXT,

  -- Event type
  event_type TEXT CHECK(event_type IN
    ('reinforcement', 'counterexample',
     'creation', 'manual_override')),

  -- Context
  scope_environment TEXT,
  scope_project TEXT,

  timestamp TEXT DEFAULT CURRENT_TIMESTAMP
);

-- Rule registry embeddings
CREATE TABLE persona_embeddings (
  rule_id TEXT PRIMARY KEY
    REFERENCES persona_rules(rule_id)
    ON DELETE CASCADE,
  embedding BLOB NOT NULL, -- 384-dim float32 array
  model_name TEXT DEFAULT
    'sentence-transformers/all-MiniLM-L6-v2',
  computed_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX idx_rules_confidence
  ON persona_rules(confidence DESC);
CREATE INDEX idx_rules_scope_env
  ON persona_rules(scope_environment);
CREATE INDEX idx_rules_scope_project
  ON persona_rules(scope_project);
CREATE INDEX idx_events_rule
  ON persona_events(rule_id, timestamp DESC);
CREATE INDEX idx_events_frame
  ON persona_events(frame_id);

-- Triggers for automatic timestamp updates
CREATE TRIGGER update_rule_timestamp
AFTER UPDATE ON persona_rules
BEGIN
  UPDATE persona_rules
  SET updated_at = CURRENT_TIMESTAMP
  WHERE rule_id = NEW.rule_id;
END;
\end{lstlisting}

\section{Classification Evaluation}

We manually labeled 200 user corrections from real development sessions and evaluated classifier accuracy.

\textbf{Dataset}:
\begin{itemize}[leftmargin=1.5em]
    \item 120 corrections matching existing rules (ground truth)
    \item 50 corrections requiring new rules
    \item 30 ambiguous corrections (could match multiple rules)
\end{itemize}

\textbf{Metrics}:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Precision}: Of corrections classified as matching rule X, what \% actually match?
    \item \textbf{Recall}: Of corrections that should match rule X, what \% were detected?
    \item \textbf{F1 Score}: Harmonic mean of precision and recall
\end{itemize}

\textbf{Results} (threshold = 0.85):
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Rule Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
Tool preferences & 0.91 & 0.83 & 0.87 \\
Communication style & 0.82 & 0.78 & 0.80 \\
Security policies & 0.95 & 0.88 & 0.91 \\
Code style & 0.79 & 0.72 & 0.75 \\
\hline
\textbf{Overall} & \textbf{0.87} & \textbf{0.80} & \textbf{0.83} \\
\hline
\end{tabular}
\end{center}

\textbf{Error analysis}:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{False positives} (12\%): Generic corrections (``that's wrong'') matched to specific rules.
    \item \textbf{False negatives} (20\%): Corrections phrased differently than registry templates (``stop doing X'' vs ``avoid X'').
\end{itemize}

\textbf{Ablation}: Lowering threshold to 0.75 improved recall to 0.88 but decreased precision to 0.79 (more false positives requiring confirmation).

\section*{Acknowledgments}

This work emerged from a collaborative design process between human, episodic memory architecture (Lex/GPT-o1), and language model reasoning (Claude Sonnet 4.5/Anthropic). Lex contributed the architectural constraints and integration design with the existing Lex/LexRunner cognitive framework. Claude Sonnet 4.5 contributed the theoretical framework for reinforcement-based procedural learning, Bayesian confidence modeling, and the reference implementation specification. Joseph M. Gustavson synthesized these contributions and is solely responsible for all claims made in this paper.

\end{document}
